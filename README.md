# Awesome-LLMs-for-Video-Understanding [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
<font size=5><center><b> Table of Contents </b> </center></font>
- [Video Understanding](#video-understanding)

- [Video Generation](#video-generation)

- [Dataset](#dataset)

- [Evaluation](#evaluation)
---
## Video Understanding
|  Title  |  Date   |   Code   |   Data   |   Venue   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**Video-LLaMA: An Instruction-Finetuned Visual Language Model for Video Understanding**](https://arxiv.org/abs/2306.02858) [![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA.svg?style=social&label=Star)](https://github.com/DAMO-NLP-SG/Video-LLaMA) | 06/2023 | [code](https://github.com/DAMO-NLP-SG/Video-LLaMA) | - |
| [**LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning**](https://arxiv.org/abs/2306.10354) [![Star](https://img.shields.io/github/stars/zjr2000/llmva-gebc.svg?style=social&label=Star)](https://github.com/zjr2000/llmva-gebc) | 06/2023 | [code](https://github.com/zjr2000/llmva-gebc) | - |
| [**VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset**](https://arxiv.org/abs/2304.08345v1) [![Star](https://img.shields.io/github/stars/TXH-mercury/VALOR.svg?style=social&label=Star)](https://github.com/TXH-mercury/VALOR) | 04/2023 | [code](https://github.com/TXH-mercury/VALOR) | - |
| [**Garbage in, garbage out: Zero-shot detection of crime using Large Language Models**](https://arxiv.org/abs/2307.06844) [![Star](https://img.shields.io/github/stars/anjsimmo/zero-shot-crime-detection.svg?style=social&label=Star)](https://github.com/anjsimmo/zero-shot-crime-detection) | 07/2023 | [code](https://github.com/anjsimmo/zero-shot-crime-detection) | - |
| **[InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation](https://arxiv.org/abs/2307.06942v1)**  | 07/2023 | [code](https://github.com/opengvlab/internvideo) | - | - |
| **[SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension](https://arxiv.org/abs/2307.16125v1)**  | 07/2023 | [code](https://github.com/ailab-cvc/seed-bench) | - | - |
| **[Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models](https://arxiv.org/abs/2306.05424)**  | 06/2023 | [code](https://github.com/mbzuai-oryx/Video-ChatGPT) | - |
| **[VALLEY: Video Assistant with Large Language model Enhanced abilitY](https://arxiv.org/abs/2306.07207)**  | 06/2023 | [code](https://github.com/RupertLuo/Valley) | - |
| **[Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration](https://arxiv.org/abs/2306.09093)**  | 06/2023 | [code](https://github.com/lyuchenyang/macaw-llm) | - |
| **[Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models](https://arxiv.org/abs/2306.11732)** | 06/2023 | - | - |
| **[MIMIC-IT: Multi-Modal In-Context Instruction Tuning](https://arxiv.org/abs/2306.05425)**  | 06/2023 | [code](https://github.com/luodian/otter) | - |
| **[Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks](https://arxiv.org/abs/2306.04362v1)**  | 06/2023 | [code](https://github.com/x-plug/youku-mplug) | - | - |
| **[FunQA: Towards Surprising Video Comprehension](https://arxiv.org/abs/2306.14899v1)**  | 06/2023 | [code](https://github.com/jingkang50/funqa) | - | - |
| **[VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset](https://arxiv.org/abs/2305.18500v1)**  | 05/2023 | [code](https://github.com/txh-mercury/vast) | - | - |
| **[VideoChat: Chat-Centric Video Understanding](https://arxiv.org/abs/2305.06355)**  | 05/2023 | [code](https://github.com/OpenGVLab/Ask-Anything) | [demo](https://huggingface.co/spaces/ynhe/AskAnything) |
| **[VideoLLM: Modeling Video Sequence with Large Language Models](https://arxiv.org/abs/2305.13292)**  | 05/2023 | [code](https://github.com/cg1177/videollm) | - |
| **[Self-Chained Image-Language Model for Video Localization and Question Answering](https://arxiv.org/abs/2305.06988v1)**  | 05/2023 | [code](https://github.com/yui010206/sevila) | - |
| **[A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot](https://arxiv.org/abs/2305.09758)** | 05/2023 | - | - |
| **[Let's Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction](https://arxiv.org/abs/2305.13903)** | 05/2023 | - | - |
| **[Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering](https://arxiv.org/abs/2304.03754)** | 04/2023 | - | - |
| **[VLog: Video as a Long Document](https://github.com/showlab/VLog)**  | 04/2023 | [demo](https://huggingface.co/spaces/TencentARC/VLog) | - |
| **[Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions](https://arxiv.org/abs/2304.04227)**  | 04/2023 | [code](https://github.com/Vision-CAIR/ChatCaptioner/tree/main/Video_ChatCaptioner) | - |
| **[ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System](https://arxiv.org/abs/2304.14407)** | 04/2023 | [project page](https://www.wangjunke.info/ChatVideo/) | - |
| **[CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos](https://arxiv.org/abs/2303.09713)**  | 03/2023 | [code](https://github.com/wade3han/champagne) | - |
| **[Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2303.01903)**  | 03/2023 | [code](https://github.com/milvlg/prophet) | - |
| **[mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video](https://arxiv.org/abs/2302.00402v1)**  | 02/2023 | [code](https://github.com/X-PLUG/mPLUG-2) | - |
| **[Learning Video Representations from Large Language Models](https://arxiv.org/abs/2212.04501)** | 12/2022 | [code](https://github.com/facebookresearch/lavila) | - |
| **[Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners](https://arxiv.org/abs/2205.10747)**  | 05/2022 | [code](https://github.com/mikewangwzhl/vidil) | - |
| **[Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598)**  | 04/2022 | [project page](https://socraticmodels.github.io/) | - |

## Video Generation
|  Title  |  Date   |   Code   |   Data   |   Venue   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**NExT-GPT: Any-to-Any Multimodal LLM**](https://arxiv.org/abs/2309.05519) [![Star](https://img.shields.io/github/stars/NExT-GPT/NExT-GPT.svg?style=social&label=Star)](https://github.com/NExT-GPT/NExT-GPT) | 09/2023 | [code](https://github.com/NExT-GPT/NExT-GPT) | - |
| [**Generative Pretraining in Multimodality**](https://arxiv.org/abs/2307.05222) [![Star](https://img.shields.io/github/stars/baaivision/emu.svg?style=social&label=Star)](https://github.com/baaivision/emu) | 07/2023 | [code](https://github.com/baaivision/emu) | - |

## Dataset
|  Title  |  Date   |   Code   |   Data   |   Venue   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**VidChapters-7M: Video Chapters at Scale**](https://arxiv.org/abs/2309.13952) [![Star](https://img.shields.io/github/stars/antoyang/VidChapters.svg?style=social&label=Star)](https://github.com/antoyang/VidChapters) | 09/2023 | [code](https://github.com/antoyang/VidChapters) | - |
| [**InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation**](https://arxiv.org/abs/2307.06942v1) [![Star](https://img.shields.io/github/stars/opengvlab/internvideo.svg?style=social&label=Star)](https://github.com/opengvlab/internvideo) | 07/2023 | [code](https://github.com/opengvlab/internvideo) | - |
| [**VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset**](https://arxiv.org/abs/2304.08345v1) [![Star](https://img.shields.io/github/stars/TXH-mercury/VALOR.svg?style=social&label=Star)](https://github.com/TXH-mercury/VALOR) | 04/2023 | [code](https://github.com/TXH-mercury/VALOR) | - |
| [**Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks**](https://arxiv.org/abs/2306.04362v1) [![Star](https://img.shields.io/github/stars/x-plug/youku-mplug.svg?style=social&label=Star)](https://github.com/x-plug/youku-mplug) | 06/2023 | [code](https://github.com/x-plug/youku-mplug) | - |
| [**VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset**](https://arxiv.org/abs/2305.18500v1) [![Star](https://img.shields.io/github/stars/txh-mercury/vast.svg?style=social&label=Star)](https://github.com/txh-mercury/vast) | 05/2023 | [code](https://github.com/txh-mercury/vast) | - |

## Evaluation
|  Title  |  Date   |   Code   |   Data   |   Venue   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**Letâ€™s Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction**](https://arxiv.org/abs/2305.13903) [![Star](https://img.shields.io/github/stars/vaishnavihimakunthala/vip.svg?style=social&label=Star)](https://github.com/vaishnavihimakunthala/vip) | 05/2023 | [code](https://github.com/vaishnavihimakunthala/vip) | - |
| [**SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension**](https://arxiv.org/abs/2307.16125v1) [![Star](https://img.shields.io/github/stars/ailab-cvc/seed-bench.svg?style=social&label=Star)](https://github.com/ailab-cvc/seed-bench) | 07/2023 | [code](https://github.com/ailab-cvc/seed-bench) | - |
| [**Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks**](https://arxiv.org/abs/2306.04362v1) [![Star](https://img.shields.io/github/stars/x-plug/youku-mplug.svg?style=social&label=Star)](https://github.com/x-plug/youku-mplug) | 07/2023 | [code](https://github.com/x-plug/youku-mplug) | - |
| [**FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation**](https://arxiv.org/abs/2311.01813) [![Star](https://img.shields.io/github/stars/llyx97/fetv.svg?style=social&label=Star)](https://github.com/llyx97/fetv) | 11/2023 | [code](https://github.com/llyx97/fetv) | - |
| [**VLM-Eval: A General Evaluation on Video Large Language Models**](https://arxiv.org/abs/2311.11865)  | 11/2023 | - | - |
