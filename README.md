# Awesome-Video-Understanding-with-LLM [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

## Methods
- **Video-LLaMA: An Instruction-Finetuned Visual Language Model for Video Understanding** [![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA.svg?style=social&label=Star)](https://github.com/DAMO-NLP-SG/Video-LLaMA)\
  [[paper]](https://arxiv.org/abs/2306.02858) [[code]](https://github.com/DAMO-NLP-SG/Video-LLaMA)

- **Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models** [![Star](https://img.shields.io/github/stars/mbzuai-oryx/Video-ChatGPT.svg?style=social&label=Star)](https://github.com/mbzuai-oryx/Video-ChatGPT)\
  ```arXiv 2023/06``` [[paper]](https://arxiv.org/abs/2306.05424) [[code]](https://github.com/mbzuai-oryx/Video-ChatGPT)

- **VALLEY: Video Assistant with Large Language model Enhanced abilitY** [![Star](https://img.shields.io/github/stars/RupertLuo/Valley.svg?style=social&label=Star)](https://github.com/RupertLuo/Valley)\
```arXiv 2023/06``` [[paper]](https://arxiv.org/abs/2306.07207) [[code]](https://github.com/RupertLuo/Valley)

- **VLog: Video as a Long Document** [![Star](https://img.shields.io/github/stars/showlab/VLog.svg?style=social&label=Star)](https://github.com/showlab/VLog)\
    [[code]](https://github.com/showlab/VLog) [[demo]](https://huggingface.co/spaces/TencentARC/VLog)

- **Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions** [![Star](https://img.shields.io/github/stars/Vision-CAIR/ChatCaptioner.svg?style=social&label=Star)](https://github.com/Vision-CAIR/ChatCaptioner)\
```arXiv 2023/04``` [[paper]](https://arxiv.org/abs/2304.04227) [[code]](https://github.com/Vision-CAIR/ChatCaptioner/tree/main/Video_ChatCaptioner)

- **ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System**\
  ```arXiv 2023/04``` [[paper]](https://arxiv.org/abs/2304.14407) [[project page]](https://www.wangjunke.info/ChatVideo/)

- **VideoChat: Chat-Centric Video Understanding** [![Star](https://img.shields.io/github/stars/OpenGVLab/Ask-Anything.svg?style=social&label=Star)](https://github.com/OpenGVLab/Ask-Anything)\
  ```arXiv 2023/05```  [[paper]](https://arxiv.org/abs/2305.06355) [[code]](https://github.com/OpenGVLab/Ask-Anything) [[demo]](https://huggingface.co/spaces/ynhe/AskAnything)

- **VideoLLM: Modeling Video Sequence with Large Language Models**\
  [[paper]](https://arxiv.org/abs/2305.13292) [[code]](https://github.com/cg1177/videollm)

- **Self-Chained Image-Language Model for Video Localization and Question Answering**\
  [[paper]](https://arxiv.org/abs/2305.06988v1) [[code]](https://github.com/yui010206/sevila)

- **VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset**\
  [[paper]](https://arxiv.org/abs/2304.08345v1) [[code]](https://github.com/TXH-mercury/VALOR) 

- **Learning Video Representations from Large Language Models**\
  [[paper]](https://arxiv.org/abs/2212.04501)

- **LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning**\
 ```arXiv 2023/06``` [[paper]](https://arxiv.org/abs/2306.10354)

- **Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language**\
  [[paper]](https://arxiv.org/abs/2204.00598) [[project page]](https://socraticmodels.github.io/)
  
- **CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos**\
  [[paper]](https://arxiv.org/abs/2303.09713)
  
- **Generative Pretraining in Multimodality**\
  [[paper]](https://arxiv.org/abs/2307.05222)

- **Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration**\
  [[paper]](https://arxiv.org/abs/2306.09093) [[code]](https://github.com/lyuchenyang/macaw-llm)

- **mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video**\
  [[paper]](https://arxiv.org/abs/2302.00402v1) [[code]](https://github.com/X-PLUG/mPLUG-2)

- **Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering**\
  [[paper]](https://arxiv.org/abs/2303.01903) [[code]](https://github.com/milvlg/prophet)

- **MIMIC-IT: Multi-Modal In-Context Instruction Tuning**\
  [[paper]](https://arxiv.org/abs/2306.05425) [[code]](https://github.com/luodian/otter)

- **Garbage in, garbage out: Zero-shot detection of crime using Large Language Models**\
  [[paper]](https://arxiv.org/abs/2307.06844) [[code]](https://github.com/anjsimmo/zero-shot-crime-detection)

- **A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot**\
  [[paper]](https://arxiv.org/abs/2305.09758)

- **Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering**\
  [[paper]](https://arxiv.org/abs/2304.03754)

- **Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models**\
  [[paper]](https://arxiv.org/abs/2306.11732)

- **Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners**\
  [[paper]](https://arxiv.org/abs/2205.10747) [[code]](https://github.com/mikewangwzhl/vidil)

- **Let's Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction**\
  [[paper]](https://arxiv.org/abs/2305.13903)


## Dataset & Metrics

- **InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation**\
  [[paper]](https://arxiv.org/pdf/2307.06942v1.pdf) [[code]](https://github.com/opengvlab/internvideo)

- **Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks**\
  [[paper]](https://arxiv.org/pdf/2306.04362v1.pdf) [[code]](https://github.com/x-plug/youku-mplug)

- **VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset**\
  [[paper]](https://arxiv.org/pdf/2305.18500v1.pdf) [[code]](https://github.com/txh-mercury/vast)

- **SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension**\
  [[paper]](https://arxiv.org/pdf/2307.16125v1.pdf) [[code]](https://github.com/ailab-cvc/seed-bench)

- **Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**\ (VideoInstruct (Video Instruction Dataset))
  [[paper]](https://arxiv.org/pdf/2306.05424v1.pdf) [[code]](https://github.com/mbzuai-oryx/video-chatgpt)

- **FunQA: Towards Surprising Video Comprehension**\
  [[paper]](https://arxiv.org/pdf/2306.14899v1.pdf) [[code]](https://github.com/jingkang50/funqa)

## Tasks
||Video Captioning|Video QA|
|--|--|--|
|Video-LLaMA|||
