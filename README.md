# Awesome-LLMs-for-Video-Understanding [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

![image](https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding/blob/main/img/milestone.png)

<font size=5><center><b> Table of Contents </b> </center></font>
- [Awesome-LLMs-for-Video-Understanding ](#awesome-llms-for-video-understanding-)
  - [Vid-LLMs: Models](#vid-llms-models)
    - [LLM-based Video Agents](#llm-based-video-agents)
    - [Video Generation](#video-generation)
    - [Vid-LLM Pretraining](#vid-llm-pretraining)
    - [Vid-LLM Instruction Tuning](#vid-llm-instruction-tuning)
      - [Fine-tuning with Connective Adapters](#fine-tuning-with-connective-adapters)
      - [Fine-tuning with Insertive Adapters](#fine-tuning-with-insertive-adapters)
      - [Fine-tuning with Hybrid Adapters](#fine-tuning-with-hybrid-adapters)
    - [Hybrid Methods](#hybrid-methods)
  - [Tasks, Datasets, and Benchmarks](#tasks-datasets-and-benchmarks)
      - [Recognition and Anticipation](#recognition-and-anticipation)
      - [Captioning and Description](#captioning-and-description)
      - [Grounding and Retrieval](#grounding-and-retrieval)
      - [Question Answering](#question-answering)



## Vid-LLMs: Models
![image](https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding/blob/main/img/timeline.png)
### LLM-based Video Agents
|  Title  |  Model   |   Date   |   Code   |   Venue  |
|:--------|:--------:|:--------:|:--------:|:--------:|
| **[Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598)**  |Socratic Models| 04/2022 | [project page](https://socraticmodels.github.io/) |arXiv|
| **[Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions](https://arxiv.org/abs/2304.04227)** | Video ChatCaptioner | 04/2023 | [code](https://github.com/Vision-CAIR/ChatCaptioner/tree/main/Video_ChatCaptioner) | arXiv |
| **[VLog: Video as a Long Document](https://github.com/showlab/VLog)**  | VLog | 04/2023 | [demo](https://huggingface.co/spaces/TencentARC/VLog) | - |
| **[ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System](https://arxiv.org/abs/2304.14407)** | ChatVideo | 04/2023 | [project page](https://www.wangjunke.info/ChatVideo/) | arXiv |
|**[MM-VID: Advancing Video Understanding with GPT-4V(ision)](https://arxiv.org/abs/2310.19773)**| 10/2023 | - | - |

### Video Generation
|  Title  |  Date   |   Code   |   Data   |   Venue   |
|:--------|:--------:|:--------:|:--------:|:--------:|


### Vid-LLM Pretraining

### Vid-LLM Instruction Tuning

#### Fine-tuning with Connective Adapters

#### Fine-tuning with Insertive Adapters

#### Fine-tuning with Hybrid Adapters


### Hybrid Methods

---

## Tasks, Datasets, and Benchmarks

#### Recognition and Anticipation
|  Title  |  Date   |   Code   |   Data   |   Venue   |
|:--------|:--------:|:--------:|:--------:|:--------:|
#### Captioning and Description

#### Grounding and Retrieval

#### Question Answering

<!-- ## Evaluation
|  Title  |  Date   |   Code   |   Data   |   Venue   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**Let’s Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction**](https://arxiv.org/abs/2305.13903) [![Star](https://img.shields.io/github/stars/vaishnavihimakunthala/vip.svg?style=social&label=Star)](https://github.com/vaishnavihimakunthala/vip) | 05/2023 | [code](https://github.com/vaishnavihimakunthala/vip) | - |
| [**SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension**](https://arxiv.org/abs/2307.16125v1) [![Star](https://img.shields.io/github/stars/ailab-cvc/seed-bench.svg?style=social&label=Star)](https://github.com/ailab-cvc/seed-bench) | 07/2023 | [code](https://github.com/ailab-cvc/seed-bench) | - |
| [**Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks**](https://arxiv.org/abs/2306.04362v1) [![Star](https://img.shields.io/github/stars/x-plug/youku-mplug.svg?style=social&label=Star)](https://github.com/x-plug/youku-mplug) | 07/2023 | [code](https://github.com/x-plug/youku-mplug) | - |
| [**FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation**](https://arxiv.org/abs/2311.01813) [![Star](https://img.shields.io/github/stars/llyx97/fetv.svg?style=social&label=Star)](https://github.com/llyx97/fetv) | 11/2023 | [code](https://github.com/llyx97/fetv) | - |
| [**VLM-Eval: A General Evaluation on Video Large Language Models**](https://arxiv.org/abs/2311.11865)  | 11/2023 | - | - | -->
